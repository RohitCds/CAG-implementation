{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ec01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "#This installs Hugging Face's transformers library, which provides pre-trained language models like GPT.\n",
    "!pip install tensorflow keras\n",
    "#Since transformers depends on TensorFlow or PyTorch, we'll need to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fb9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now show you how the code and output for a normal propmt based text generation model would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5d28cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time and place, you see the presence of animals, animals and plants.…\n",
      "\n",
      "The great dragon-like beasts, who sometimes have large bodies, are as close to the human eye as possible.\n",
      "A humanoid creature, a humanoid animal, has a very large body, which is about 4 to 5 feet tall. A humanoid being with a light-vision range of 30 feet, weighs about 1,000 pounds. The creature can be seen in a group or in some\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# pipeline is a high-level function from transformers that makes using pre-trained models easy.\n",
    "#\"text-generation\" tells pipeline we want a model specialized in generating text.\n",
    "#\"distilgpt2\" is a lightweight version of GPT-2, making it faster.\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", framework=\"pt\")\n",
    "\n",
    "def generate_text(prompt, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates text based on the given prompt.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): The input text.\n",
    "    - max_length (int): Max length of the generated text.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Generated text.\n",
    "    \"\"\"\n",
    "    result = generator(\"Once upon a time and place,\", max_length=100, temperature=0.7, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    return result[0]['generated_text']\n",
    "# The output of this function is in the form of a list and hence we are returning \"result[0]\"\n",
    "#This function takes a prompt and uses the generator model to generate text.\n",
    "#max_length=50 ensures the response isn’t too long.\n",
    "#num_return_sequences=1 means we only generate one response at a time.\n",
    "\n",
    "# Test the function\n",
    "prompt = \"Once upon a time, in a faraway kingdom, a young hero embarked on a journey to find the lost treasure. He...\"\n",
    "\n",
    "print(generate_text(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b1acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our goal here is to hash the prompt (basically create a unique identifier for the prompt) and map it to its result creating \n",
    "# a sort of lookup that can be used by the function to avoid regeneration of responses for the same prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e699a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a faraway kingdom, a young hero named Alex embarked on a journey to find the lost treasure. On his journey, he encountered a strange forest. As he entered, he saw...\n",
      "\n",
      "A young man standing in the dark, one of the most beautiful creatures in existence.\n",
      "Alex's name was Alex. He was a man who had made life a reality. And he knew the truth\n",
      "\n",
      "\n",
      "Fetching from cache...\n",
      "Once upon a time, in a faraway kingdom, a young hero named Alex embarked on a journey to find the lost treasure. On his journey, he encountered a strange forest. As he entered, he saw...\n",
      "\n",
      "A young man standing in the dark, one of the most beautiful creatures in existence.\n",
      "Alex's name was Alex. He was a man who had made life a reality. And he knew the truth\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "# used for hashing functions\n",
    "# We create a cache dictionary to store pairs of prompts and their previously generated responses. \n",
    "cache = {}\n",
    "\n",
    "def generate_text_with_cache(prompt, max_length=50):\n",
    "    \"\"\"\n",
    "    Generates text based on the given prompt with a caching mechanism.\n",
    "    \n",
    "    Args:\n",
    "    - prompt (str): The input text for the model.\n",
    "    - max_length (int): Max length of the generated text (default is 50).\n",
    "    \n",
    "    Returns:\n",
    "    - str: The generated text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We first hash the prompt to create a unique key for caching\n",
    "    # We use MD5 hash function to create a unique identifier for the prompt\n",
    "    # This helps ensure that the same prompt always generates the same hash key\n",
    "    prompt_hash = hashlib.md5(prompt.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # we check if the result is already in the cache\n",
    "    # If the hashed prompt is found in the cache, we return the cached result\n",
    "    if prompt_hash in cache:\n",
    "        print(\"Fetching from cache...\")  # We print a message to indicate that the result is coming from the cache\n",
    "        return cache[prompt_hash]  \n",
    "    \n",
    "    # If the result is not in the cache, generate new text using the model\n",
    "    # Call the model’s pipeline with the prompt and other settings like max_length\n",
    "    result = generator(prompt, max_length= max_length + len(prompt.split()), temperature=0.7, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    \n",
    "    # we adjust the \"max_length\" of the response based on the prompt length\n",
    "    # The result is a list, so we take the first generated response\n",
    "    generated_text = result[0]['generated_text']\n",
    "    \n",
    "    # Store the generated text in the cache with the hashed prompt as the key\n",
    "    # This will allow us to avoid recomputing for the same prompt in the future\n",
    "    cache[prompt_hash] = generated_text\n",
    "    \n",
    "    # Return the generated text\n",
    "    return generated_text\n",
    "\n",
    "# Test the function with caching\n",
    "prompt = \"Once upon a time, in a faraway kingdom, a young hero named Alex embarked on a journey to find the lost treasure. On his journey, he encountered a strange forest. As he entered, he saw...\"\n",
    "print(generate_text_with_cache(prompt))  # This will generate new text\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(generate_text_with_cache(prompt))  # This call should fetch from cache, not generate new text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd96271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the first output was freshly generated by the model but when we ran it with the same prompt again, it was \n",
    "# able to figure out that it had come across the same prompt previously and instead of utilizing resources in generating \n",
    "# another response for the same prompt, the function pulled the previous response from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74542543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It goes without saying that the output generated by the snippets feels a bit out of place and could be more coherent.\n",
    "# This can be dealth with by playing around with the model settings (temperature, max length, etc) but since the purpose of\n",
    "# this project is to display a basic implementation of CAG, the actual content of the output has little to do with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
